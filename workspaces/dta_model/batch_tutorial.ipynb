{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import create_dta_toy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887d9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25\n"
     ]
    }
   ],
   "source": [
    "fasta_vocab_path = \"fasta_vocab.json\"\n",
    "with open(fasta_vocab_path, 'r') as f:\n",
    "    fasta_vocab = json.load(f)\n",
    "\n",
    "fasta_vocab_size = len(fasta_vocab)\n",
    "print(\"Vocab size:\", fasta_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f59434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 101\n"
     ]
    }
   ],
   "source": [
    "smiles_vocab_path = \"vocab_chars.json\"\n",
    "with open(smiles_vocab_path, 'r') as f:\n",
    "    smiles_vocab = json.load(f)\n",
    "\n",
    "smiles_vocab_size = len(smiles_vocab)\n",
    "print(\"Vocab size:\", smiles_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_onehot(smiles: str, vocab: dict, max_len: int):\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot = np.zeros((max_len, vocab_size), dtype=np.float32)\n",
    "    pad_id = vocab.get('<pad>', 0)\n",
    "    unk_id = vocab.get('<unk>', 3)\n",
    "    \n",
    "    for i, ch in enumerate(smiles[:max_len]):\n",
    "        idx = vocab.get(ch, unk_id)\n",
    "        one_hot[i, idx] = 1.0\n",
    "        \n",
    "    for i in range(len(smiles), max_len):\n",
    "        one_hot[i, pad_id] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_onehot(fasta: str, vocab: dict, max_len: int):\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot = np.zeros((max_len, vocab_size), dtype=np.float32)\n",
    "    pad_id = vocab.get('<pad>')\n",
    "    unk_id = vocab.get('<unk>')\n",
    "\n",
    "    for i, ch in enumerate(fasta[:max_len]):\n",
    "        idx = vocab.get(ch.upper(), unk_id)\n",
    "        one_hot[i, idx] = 1.0\n",
    "\n",
    "    for i in range(len(fasta), max_len):\n",
    "        one_hot[i, pad_id] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61f067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DrugTargetAffinityModel(\n",
       "  (smiles_encoder): SequenceTransformerEncoder(\n",
       "    (embedding): Linear(in_features=101, out_features=128, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fasta_encoder): SequenceTransformerEncoder(\n",
       "    (embedding): Linear(in_features=25, out_features=128, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regression_head): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_dta_toy_model(\n",
    "    smiles_vocab_size=smiles_vocab_size, \n",
    "    fasta_vocab_size=fasta_vocab_size\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab10ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SMILES batch input: smiles_batch_input.bin (Shape: (2, 256, 101))\n",
      "Saved FASTA batch input:  fasta_batch_input.bin (Shape: (2, 1000, 25))\n"
     ]
    }
   ],
   "source": [
    "smiles_seq_len = 256\n",
    "fasta_seq_len = 1000\n",
    "\n",
    "smiles_example_1 = \"CC[NH+](CC)[C@](C)(CC)[C@H](O)c1cscc1Br\"\n",
    "fasta_example_1 = \"MEECWVTEIANGSKDGLDSNPMKDYMILSGPQKTAVAVLCTLLGLLSALENVAVLYLILSSHQLRRKPSYLFIGSLAGADFLASVVFACSFVNFHVFHGVDSKAVFLLKIGSVTMTFTASVGSLLLTAIDRYLCLRYPPSYKALLTRGRALVTLGIMWVLSALVSYLPLMGWTCCPRPCSELFPLIPNDYLLSWLLFIAFLFSGIIYTYGHVLWKAHQHVASLSGHQDRQVPGMARMRLDVRLAKTLGLVLAVLLICWFPVLALMAHSLATTLSDQVKKAFAFCSMLCLINSMVNPVIYALRSGEIRSSAHHCLAHWKKCVRGLGSEAKEEAPRSSVTETEADGKITPWPDSRDLDLSDC\"\n",
    "\n",
    "smiles_example_2 = \"CN1CCN(CC(=O)N(C)c2ccc(NC(=C3C(=O)Nc4ccccc43)c3ccccc3)cc2)CC1\"\n",
    "fasta_example_2 = \"MGHALCVCSRGTVIIDNKRYLFIQKLGEGGFSYVDLVEGLHDGHFYALKRILCHEQQDREEAQREADMHRLFNHPNILRLVAYCLRERGAKHEAWLLLPFFKRGTLWNEIERLKDKGNFLTEDQILWLLLGICRGLEAIHAKGYAHRDLKPTNILLGDEGQPVLMDLGSMNQACIHVEGSRQALTLQDWAAQRCTISYRAPELFSVQSHCVIGERTDVWSLGCVLYAMMFGEGPYDMVFQKGDSVALAVQNQLSIPQSPRHSSALRQLLNSMMTVDPHQRPHIPLLLSQLEALQPPAPGQHTTQIEKAAC\"\n",
    "\n",
    "smiles_onehot_1 = smiles_to_onehot(smiles_example_1, smiles_vocab, max_len=smiles_seq_len)\n",
    "fasta_onehot_1 = fasta_to_onehot(fasta_example_1, fasta_vocab, max_len=fasta_seq_len)\n",
    "\n",
    "smiles_onehot_2 = smiles_to_onehot(smiles_example_2, smiles_vocab, max_len=smiles_seq_len)\n",
    "fasta_onehot_2 = fasta_to_onehot(fasta_example_2, fasta_vocab, max_len=fasta_seq_len)\n",
    "\n",
    "smiles_batch_np = np.stack([smiles_onehot_1, smiles_onehot_2]).astype(np.float32)\n",
    "fasta_batch_np = np.stack([fasta_onehot_1, fasta_onehot_2]).astype(np.float32)\n",
    "\n",
    "smiles_bin_path = \"smiles_batch_input.bin\"\n",
    "fasta_bin_path = \"fasta_batch_input.bin\"\n",
    "\n",
    "smiles_batch_np.tofile(smiles_bin_path)\n",
    "fasta_batch_np.tofile(fasta_bin_path)\n",
    "\n",
    "print(f\"Saved SMILES batch input: {smiles_bin_path} (Shape: {smiles_batch_np.shape})\")\n",
    "print(f\"Saved FASTA batch input:  {fasta_bin_path} (Shape: {fasta_batch_np.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc774cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42b17a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
