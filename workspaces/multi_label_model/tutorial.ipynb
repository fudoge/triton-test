{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2551005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import create_dta_toy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2887d9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25\n"
     ]
    }
   ],
   "source": [
    "fasta_vocab_path = \"fasta_vocab.json\"\n",
    "with open(fasta_vocab_path, 'r') as f:\n",
    "    fasta_vocab = json.load(f)\n",
    "\n",
    "fasta_vocab_size = len(fasta_vocab)\n",
    "print(\"Vocab size:\", fasta_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f59434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 101\n"
     ]
    }
   ],
   "source": [
    "smiles_vocab_path = \"vocab_chars.json\"\n",
    "with open(smiles_vocab_path, 'r') as f:\n",
    "    smiles_vocab = json.load(f)\n",
    "\n",
    "smiles_vocab_size = len(smiles_vocab)\n",
    "print(\"Vocab size:\", smiles_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dbc9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_onehot(smiles: str, vocab: dict, max_len: int):\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot = np.zeros((max_len, vocab_size), dtype=np.float32)\n",
    "    pad_id = vocab.get('<pad>', 0)\n",
    "    unk_id = vocab.get('<unk>', 3)\n",
    "    \n",
    "    for i, ch in enumerate(smiles[:max_len]):\n",
    "        idx = vocab.get(ch, unk_id)\n",
    "        one_hot[i, idx] = 1.0\n",
    "        \n",
    "    for i in range(len(smiles), max_len):\n",
    "        one_hot[i, pad_id] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343d2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_onehot(fasta: str, vocab: dict, max_len: int):\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot = np.zeros((max_len, vocab_size), dtype=np.float32)\n",
    "    pad_id = vocab.get('<pad>')\n",
    "    unk_id = vocab.get('<unk>')\n",
    "\n",
    "    for i, ch in enumerate(fasta[:max_len]):\n",
    "        idx = vocab.get(ch.upper(), unk_id)\n",
    "        one_hot[i, idx] = 1.0\n",
    "\n",
    "    for i in range(len(fasta), max_len):\n",
    "        one_hot[i, pad_id] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ab10ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES Input Shape: torch.Size([1, 256, 101])\n",
      "FASTA Input Shape:  torch.Size([1, 1000, 25])\n",
      "Output Shape:       torch.Size([1, 2])\n",
      "Predicted Values:   [0.42003753781318665, 0.2855313718318939]\n"
     ]
    }
   ],
   "source": [
    "model = create_dta_toy_model(\n",
    "    smiles_vocab_size=smiles_vocab_size, \n",
    "    fasta_vocab_size=fasta_vocab_size\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "smiles_seq_len = 256\n",
    "fasta_seq_len = 1000\n",
    "smiles_example = \"CC[NH+](CC)[C@](C)(CC)[C@H](O)c1cscc1Br\"\n",
    "fasta_example = \"MEECWVTEIANGSKDGLDSNPMKDYMILSGPQKTAVAVLCTLLGLLSALENVAVLYLILSSHQLRRKPSYLFIGSLAGADFLASVVFACSFVNFHVFHGVDSKAVFLLKIGSVTMTFTASVGSLLLTAIDRYLCLRYPPSYKALLTRGRALVTLGIMWVLSALVSYLPLMGWTCCPRPCSELFPLIPNDYLLSWLLFIAFLFSGIIYTYGHVLWKAHQHVASLSGHQDRQVPGMARMRLDVRLAKTLGLVLAVLLICWFPVLALMAHSLATTLSDQVKKAFAFCSMLCLINSMVNPVIYALRSGEIRSSAHHCLAHWKKCVRGLGSEAKEEAPRSSVTETEADGKITPWPDSRDLDLSDC\"\n",
    "\n",
    "smiles_onehot = smiles_to_onehot(smiles_example, smiles_vocab, max_len=smiles_seq_len)\n",
    "fasta_onehot = fasta_to_onehot(fasta_example, fasta_vocab, max_len=fasta_seq_len)\n",
    "\n",
    "smiles_tensor = torch.from_numpy(smiles_onehot).unsqueeze(0)\n",
    "fasta_tensor = torch.from_numpy(fasta_onehot).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(smiles_tensor, fasta_tensor)\n",
    "\n",
    "print(f\"SMILES Input Shape: {smiles_tensor.shape}\")\n",
    "print(f\"FASTA Input Shape:  {fasta_tensor.shape}\")\n",
    "print(f\"Output Shape:       {output.shape}\")\n",
    "print(f\"Predicted Values:   {output.squeeze().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf968b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_smiles = torch.zeros(1, smiles_seq_len, smiles_vocab_size, dtype=torch.float32)\n",
    "dummy_fasta = torch.zeros(1, fasta_seq_len, fasta_vocab_size, dtype=torch.float32)\n",
    "\n",
    "traced_model = torch.jit.trace(model, (dummy_smiles, dummy_fasta), strict=False, check_trace=False)\n",
    "\n",
    "traced_path = \"model.pt\"\n",
    "traced_model.save(traced_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d576fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Triton config file: config.pbtxt\n",
      "\n",
      "--- config.pbtxt content ---\n",
      "name: \"dta_model_output2\"\n",
      "platform: \"pytorch_libtorch\"\n",
      "max_batch_size: 16\n",
      "\n",
      "input [\n",
      "  {\n",
      "    name: \"input__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [256, 101]\n",
      "  },\n",
      "  {\n",
      "    name: \"input__1\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [1000, 25]\n",
      "  }\n",
      "]\n",
      "\n",
      "output [\n",
      "  {\n",
      "    name: \"output__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    # === 변경된 부분: 출력 차원을 1에서 2로 수정 ===\n",
      "    dims: [2]\n",
      "  }\n",
      "]\n",
      "\n",
      "instance_group [\n",
      "  {\n",
      "    count: 1\n",
      "    kind: KIND_CPU\n",
      "  }\n",
      "]\n",
      "\n",
      "dynamic_batching {\n",
      "  preferred_batch_size: [4, 8, 16]\n",
      "  max_queue_delay_microseconds: 100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Triton Inference Server용 config.pbtxt 파일 생성 (출력 2개)\n",
    "\n",
    "model_name = \"dta_model_output2\" # 모델 이름 변경 권장\n",
    "platform = \"pytorch_libtorch\"\n",
    "max_batch_size = 16\n",
    "\n",
    "# 이 셀을 실행하기 전에 smiles_seq_len, smiles_vocab_size, fasta_seq_len, fasta_vocab_size 변수가\n",
    "# 노트북의 이전 셀에서 정의되어 있어야 합니다.\n",
    "\n",
    "config_content = f\"\"\"name: \"{model_name}\"\n",
    "platform: \"{platform}\"\n",
    "max_batch_size: {max_batch_size}\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [{smiles_seq_len}, {smiles_vocab_size}]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [{fasta_seq_len}, {fasta_vocab_size}]\n",
    "  }}\n",
    "]\n",
    "\n",
    "output [\n",
    "  {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    # === 변경된 부분: 출력 차원을 1에서 2로 수정 ===\n",
    "    dims: [2]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    count: 1\n",
    "    kind: KIND_CPU\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  preferred_batch_size: [4, 8, 16]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# 파일로 저장\n",
    "config_path = \"config.pbtxt\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"Saved Triton config file: {config_path}\")\n",
    "print(\"\\n--- config.pbtxt content ---\")\n",
    "print(config_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b4fb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SMILES example input: smiles_input.bin (Shape: (256, 101))\n",
      "Saved FASTA example input:  fasta_input.bin (Shape: (1000, 25))\n"
     ]
    }
   ],
   "source": [
    "smiles_input_np = smiles_onehot.astype(np.float32)\n",
    "fasta_input_np = fasta_onehot.astype(np.float32)\n",
    "\n",
    "smiles_bin_path = \"smiles_input.bin\"\n",
    "fasta_bin_path = \"fasta_input.bin\"\n",
    "\n",
    "smiles_input_np.tofile(smiles_bin_path)\n",
    "fasta_input_np.tofile(fasta_bin_path)\n",
    "\n",
    "print(f\"Saved SMILES example input: {smiles_bin_path} (Shape: {smiles_input_np.shape})\")\n",
    "print(f\"Saved FASTA example input:  {fasta_bin_path} (Shape: {fasta_input_np.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42b17a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
